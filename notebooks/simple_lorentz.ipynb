{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.autograd import grad\n",
    "\n",
    "import einops\n",
    "import math\n",
    "\n",
    "sys.path.append('../python')\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from Lorentz_loss import LorentzLoss\n",
    "sys.path.append('..')\n",
    "\n",
    "\n",
    " \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A very simple test of what we've done so far"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we define a Lorentz scalar function :\n",
    "\n",
    "\n",
    "$$\n",
    "f(x) = \\frac{1}{\\sqrt{20}}\\left(z+\\frac{z^4}{4}-1\\right)+1\\;\\;,\\;\\;z = x_\\mu\\eta^{\\mu\\nu}x_\\nu = t^2-\\boldsymbol{x}^2\n",
    "$$\n",
    "\n",
    "The reason for this shape is that for $x_\\mu\\sim{\\cal N}(0,1)$, $f(x)$ will be a random variable with a mean and variance of 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scalar_val(x):\n",
    "    # assume shape [..., N, 4]\n",
    "    if x.shape[-1]!=4:\n",
    "        raise ValueError('last dim should be 4')\n",
    "    eta = torch.tensor([1, -1, -1, -1])\n",
    "    xx = torch.einsum('... i, i,... i -> ... ', x, eta, x)\n",
    "    return (xx + xx**2/4-1) / math.sqrt(20)+1\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((tensor(0.9963), tensor(1.0042)),\n",
       " torch.Size([100, 1000, 4]),\n",
       " torch.Size([100, 1000]))"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# generate data and check mean/var\n",
    "num_vec = 1000\n",
    "# Generate data and labels\n",
    "data = torch.randn([100,num_vec, 4])  # Using a batch size of 10\n",
    "labels = scalar_val(data)\n",
    "\n",
    "torch.var_mean(scalar_val(data)), data.shape,labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a model\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, d_in, d_out, bias=True):\n",
    "        super(Block, self).__init__()\n",
    "        self.fc = nn.Linear(d_in, d_out, bias=bias)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.fc(x)\n",
    "        x = nn.functional.selu(x)\n",
    "        return x\n",
    "\n",
    "class CrazyModel(nn.Module):\n",
    "    def __init__(self,n_hid=2,d_hid=10):\n",
    "        super(CrazyModel, self).__init__()\n",
    "        modules = [Block(4, d_hid)]\n",
    "        modules += [Block(d_hid,d_hid) for _ in range(n_hid)]\n",
    "        modules += [Block(d_hid,1)]\n",
    "\n",
    "        self.blocks = nn.Sequential(*modules)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.blocks(x).squeeze()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model, loss, and optimizer\n",
    "\n",
    "\n",
    "model = CrazyModel()\n",
    "model.train()\n",
    "cirterion = nn.MSELoss(reduction = 'mean')\n",
    "\n",
    "learning_rate = 3e-2\n",
    "weight_deacy  = 1e-3\n",
    "\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate,weight_decay=weight_deacy)\n",
    "\n",
    "\n",
    "# data loader\n",
    "batch_size = 64  # Number of samples per batch\n",
    "dataset = torch.utils.data.TensorDataset(data, labels)\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter: 100\t|\tloss: 0.9478254914283752\n",
      "iter: 500\t|\tloss: 0.5768612027168274\n",
      "iter: 1000\t|\tloss: 0.4849708080291748\n",
      "iter: 2000\t|\tloss: 0.40425583720207214\n",
      "iter: 3000\t|\tloss: 0.27159059047698975\n",
      "iter: 4000\t|\tloss: 0.18058785796165466\n",
      "iter: 5000\t|\tloss: 0.1503157615661621\n",
      "iter: 6000\t|\tloss: 0.14825136959552765\n",
      "iter: 7000\t|\tloss: 0.13348020613193512\n",
      "iter: 8000\t|\tloss: 0.13562260568141937\n",
      "iter: 9000\t|\tloss: 0.128230020403862\n"
     ]
    }
   ],
   "source": [
    "# Initialize model, loss, and optimizer\n",
    "iters = 10000\n",
    "\n",
    "for iter in range(iters):\n",
    "    for batch_data, batch_labels in dataloader:\n",
    "        preds = model(batch_data)\n",
    "        loss = cirterion(preds, batch_labels)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    if (iter % 1000 == 0 and iter > 0) or iter == 100 or iter == 500:\n",
    "        print(f'iter: {iter}\\t|\\tloss: {loss.item()}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lasso",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
