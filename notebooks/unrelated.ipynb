{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import einops\n",
    "import math\n",
    "\n",
    "\n",
    "\n",
    "class simplenet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(simplenet,self).__init__()\n",
    "        self.vecs=nn.Parameter(torch.randn([12],requires_grad=True))\n",
    "        \n",
    "    \n",
    "    def forward(self):\n",
    "        a,b,c,d=self.vecs.chunk(4)\n",
    "        return torch.einsum('i,j->ij',a,b)+torch.einsum('i,j->ij',c,d)\n",
    "    \n",
    "\n",
    "class simplernet(nn.Module):\n",
    "    def __init__(self,n):\n",
    "        super(simplernet,self).__init__()\n",
    "        self.a=nn.Parameter(torch.randn([n],requires_grad=True)/1.0)\n",
    "        self.b=nn.Parameter(torch.randn([n],requires_grad=True)/1.0)\n",
    "        self.c=nn.Parameter(torch.randn([n],requires_grad=True)/1.0)\n",
    "        self.d=nn.Parameter(torch.randn([n],requires_grad=True)/1.0)\n",
    "        \n",
    "    \n",
    "    def forward(self):\n",
    "        return torch.einsum('i,j->ij',self.a,self.b)+torch.einsum('i,j->ij',self.c,self.d)\n",
    "    \n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "class biggerNet(nn.Module):\n",
    "    def __init__(self,dim,num):\n",
    "        super(biggerNet,self).__init__()\n",
    "        self.cols=nn.Parameter(torch.randn([num,dim],requires_grad=True,dtype=torch.float32)/math.sqrt(num))\n",
    "        self.rows=nn.Parameter(torch.randn([num,dim],requires_grad=True,dtype=torch.float32)/math.sqrt(num))\n",
    "\n",
    "    def forward(self):\n",
    "        outer=torch.einsum('ij, ik -> ijk',self.cols,self.rows)\n",
    "        return outer.sum(dim=0)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.26230884, 0.3357203 , 0.2036532 , 0.39317602, 0.34981224,\n",
       "       0.23194468, 0.35149246, 0.2701878 , 0.28628075, 0.55980575,\n",
       "       0.3431646 , 0.1895501 , 0.20602353, 0.48316655, 0.3648572 ,\n",
       "       0.45552692, 0.10852868, 0.41295424, 0.6547695 , 0.2648464 ],\n",
       "      dtype=float32)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model=biggerNet(3,20)\n",
    "model()\n",
    "(model.rows.norm(2,dim=1)*model.cols.norm(2,dim=1)).sqrt().detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 30000\t|\tloss: -12.38\t|\tnorms: [ 0.3  0.1  0.3  0.3  0.2  0.4  0.2 -0.3 -0.2  0.2]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tqdm import trange,tqdm\n",
    "torch.manual_seed(7923)\n",
    "\n",
    "model=biggerNet(12,10)\n",
    "\n",
    "K=torch.randn([12,1],requires_grad=False)\n",
    "Q=torch.randn([12,1],requires_grad=False)\n",
    "Teach=Q @ K.T\n",
    "\n",
    "K=torch.randn([12,1],requires_grad=False)\n",
    "Q=torch.randn([12,1],requires_grad=False)\n",
    "\n",
    "Teach += Q @ K.T\n",
    "\n",
    "optimizer=torch.optim.AdamW(model.parameters(),lr=0.05,weight_decay=0*1e-1,amsgrad=True)\n",
    "\n",
    "epochs=30000\n",
    "\n",
    "\n",
    "norm_arr=[]\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    loss=torch.norm(model()-Teach,2)**2\n",
    "    # print(loss)\n",
    "    # loss+=0.01*torch.norm(model.vecs,1)\n",
    "    # norms=torch.norm(model.vecs.view(4,3),2,dim=1).view(2,2).prod(dim=1).sqrt()\n",
    "    # norms=[(model.a.norm()*model.b.norm()),(model.c.norm()*model.d.norm())]\n",
    "    # norms=[(model.b.norm()),(model.d.norm())]\n",
    "    # loss+=0.1* torch.tensor(norms).sum()\n",
    "    norms=(model.rows.norm(2,dim=1)*model.cols.norm(2,dim=1)).sqrt().detach().numpy()\n",
    "    norm_arr.append(norms)\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "\n",
    "    if (epoch+1)%30000==0:\n",
    "        print(f'epoch: {epoch+1}\\t|\\tloss: {loss.log10().item():.2f}\\t|\\tnorms: '+np.array2string(np.log10(norms), precision=1, floatmode='fixed'))#\\t|\\tnorms: {norms[0]:.2f}, {norms[1]:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "for line in np.array(norm_arr).T:\n",
    "    plt.plot(line)\n",
    "\n",
    "plt.yscale('log')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Norms')\n",
    "# plt.ylim(1e-5,10)\n",
    "plt.xlim(0,20000)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([   -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,\n",
       "           -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,\n",
       "           -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,\n",
       "           -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,\n",
       "           -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,\n",
       "           -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,\n",
       "           -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,\n",
       "           -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,\n",
       "           -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,\n",
       "           -inf,    -inf, -7.2247, -7.2247, -7.2247, -7.2247, -7.2247, -7.2247,\n",
       "        -7.2247, -7.2247, -7.2247, -7.2247, -7.2247, -7.2247, -7.2247, -7.2247,\n",
       "        -7.2247, -6.9237, -6.9237, -6.9237, -6.9237, -6.9237, -6.9237, -6.9237,\n",
       "        -6.9237, -6.9237, -6.9237, -6.9237, -6.9237, -6.9237, -6.9237, -6.9237,\n",
       "        -6.9237, -6.9237, -6.9237, -6.9237, -6.9237, -6.9237, -6.9237, -6.9237,\n",
       "        -6.9237, -6.9237, -6.9237, -6.9237, -6.9237, -6.9237, -6.9237, -6.9237,\n",
       "        -6.9237, -6.9237, -6.7476, -6.7476, -6.7476, -6.7476, -6.6227, -6.6227,\n",
       "        -6.6227, -6.6227, -6.5257, -6.4466, -6.4466, -6.4466, -6.4466, -6.3796,\n",
       "        -6.3216, -6.2247, -6.1833, -6.1833, -6.1455, -5.9237, -5.4844, -5.4765])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "(1-model().detach()/Teach).abs().log10().view(-1).sort().values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class short(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(short,self).__init__()\n",
    "        self.y=nn.Parameter(torch.randn([1],requires_grad=True,dtype=torch.double))\n",
    "        self.x=nn.Parameter(torch.randn([1],requires_grad=True,dtype=torch.double))\n",
    "       \n",
    "        self.t1=nn.Parameter(torch.randn([1],requires_grad=True,dtype=torch.double))\n",
    "        self.t2=nn.Parameter(torch.randn([1],requires_grad=True,dtype=torch.double))\n",
    "        self.q1=nn.Parameter(torch.randn([1],requires_grad=True,dtype=torch.double))\n",
    "        self.q2=nn.Parameter(torch.randn([1],requires_grad=True,dtype=torch.double))\n",
    "        \n",
    "    \n",
    "    def forward(self,input):\n",
    "        T1=torch.cos(self.t1)\n",
    "        T2=torch.cos(self.t2)\n",
    "        Q1=torch.cos(self.q1)\n",
    "        Q2=torch.cos(self.q2)\n",
    "\n",
    "        T12=torch.cos(self.t1-self.t2)\n",
    "        Q12=torch.cos(self.q1-self.q2)\n",
    "\n",
    "        return 1+self.x**2+self.y**2-2*self.x*T1*Q1-2*self.y*T2*Q2+2*self.x*self.y*T12*Q12\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0848221364009287\n",
      "1.0848221364009287\n",
      "1.0848221364009287\n",
      "1.0848221364009287\n",
      "1.0848221364009287\n",
      "1.0848221364009287\n",
      "1.0848221364009287\n",
      "1.0848221364009287\n",
      "1.0848221364009287\n",
      "1.0848221364009287\n",
      "1.0848221364009287\n",
      "1.0848221364009287\n",
      "1.0848221364009287\n",
      "1.0848221364009287\n",
      "1.0848221364009287\n",
      "1.0848221364009287\n",
      "1.0848221364009287\n",
      "1.0848221364009287\n",
      "1.0848221364009287\n",
      "1.0848221364009287\n"
     ]
    }
   ],
   "source": [
    "model=short()\n",
    "\n",
    "optimizer=torch.optim.SGD(model.parameters(),lr= 10)\n",
    "\n",
    "for i in range(20):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    loss=model(1).sum()\n",
    "    \n",
    "    optimizer.step()\n",
    "\n",
    "    print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y -4.382595385615131 -0.011813331989359764\n",
      "x -11.365170701502864 -0.04147588133806347\n",
      "t1 -0.12328021247151669 0.27765189142157204\n",
      "t2 -0.13743348349124382 1.2311498137139976\n",
      "q1 -0.17087148749312517 0.3617193944205535\n",
      "q2 0.0057330334781476155 -0.09005317162602089\n"
     ]
    }
   ],
   "source": [
    "# model=short()\n",
    "model.train()\n",
    "loss=model(None).sum()\n",
    "loss.backward()\n",
    "oprimizer.step()\n",
    "for name, param in model.named_parameters():\n",
    "    print(name, param.grad.item(), param.item())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0232\n",
      "0.019000000000000156\n",
      "0.014736000000000067\n",
      "0.013224000000000154\n",
      "0.011688959999999776\n",
      "0.011144639999999923\n",
      "0.010592025600000433\n",
      "0.010396070400000084\n",
      "0.010197129215999743\n",
      "0.010126585344000003\n",
      "0.01005496651776015\n",
      "0.010029570723840135\n",
      "0.01000378794639368\n",
      "0.00999464546058247\n",
      "0.009985363660701527\n",
      "0.009982072365809756\n",
      "0.009978730917852601\n",
      "0.00997754605169131\n",
      "0.009976343130426862\n",
      "0.009975916578609145\n",
      "0.00997548352695359\n",
      "0.009975329968299234\n",
      "0.009975174069703498\n",
      "0.009975118788587672\n",
      "0.009975062665093109\n",
      "0.009975042763891698\n",
      "0.00997502255943356\n",
      "0.009975015395000956\n",
      "0.009975008121396118\n",
      "0.009975005542200327\n",
      "0.009975002923702575\n",
      "0.009975001995192018\n",
      "0.009975001052533008\n",
      "0.009975000718269168\n",
      "0.009975000378911842\n",
      "0.00997500025857712\n",
      "0.00997500013640826\n",
      "0.00997500009308744\n",
      "0.009975000049106892\n",
      "0.00997500003351165\n",
      "0.009975000017678506\n",
      "0.00997500001206409\n",
      "0.009975000006364433\n",
      "0.009975000004343057\n",
      "0.009975000002291082\n",
      "0.009975000001563546\n",
      "0.009975000000824748\n",
      "0.00997500000056271\n",
      "0.009975000000296903\n",
      "0.009975000000202662\n",
      "0.009975000000106983\n",
      "0.009975000000072915\n",
      "0.009975000000038465\n",
      "0.009975000000026312\n",
      "0.009975000000013773\n",
      "0.009975000000009613\n",
      "0.009975000000005004\n",
      "0.009975000000003254\n",
      "0.009975000000001946\n",
      "0.009975000000001158\n",
      "0.009975000000000676\n",
      "0.009975000000000558\n",
      "0.00997500000000018\n",
      "0.009975000000000242\n",
      "0.009975000000000076\n",
      "0.009975000000000067\n",
      "0.009975000000000062\n",
      "0.00997499999999988\n",
      "0.00997499999999997\n",
      "0.009975000000000038\n",
      "0.009974999999999956\n",
      "0.009975000000000043\n",
      "0.009974999999999862\n",
      "0.009974999999999868\n",
      "0.009975000000000116\n",
      "0.009975000000000032\n",
      "0.009974999999999913\n",
      "0.009974999999999862\n",
      "0.009975000000000012\n",
      "0.00997499999999985\n",
      "0.009974999999999986\n",
      "0.009975000000000065\n",
      "0.00997499999999988\n",
      "0.009974999999999927\n",
      "0.00997499999999986\n",
      "0.009974999999999979\n",
      "0.009975000000000026\n",
      "0.009975000000000008\n",
      "0.009974999999999993\n",
      "0.009975000000000116\n",
      "0.009975000000000062\n",
      "0.009975000000000133\n",
      "0.009975000000000149\n",
      "0.009975000000000013\n",
      "0.009975000000000154\n",
      "0.009974999999999984\n",
      "0.00997499999999998\n",
      "0.009974999999999925\n",
      "0.009974999999999876\n",
      "0.00997499999999999\n",
      "0.00997500000000013\n",
      "0.009975000000000149\n",
      "0.009975000000000145\n",
      "0.00997500000000011\n",
      "0.009975000000000107\n",
      "0.009974999999999998\n",
      "0.00997500000000004\n",
      "0.00997499999999993\n",
      "0.009974999999999913\n",
      "0.009974999999999935\n",
      "0.009975000000000013\n",
      "0.009975000000000114\n",
      "0.009975000000000029\n",
      "0.009974999999999956\n",
      "0.009975000000000128\n",
      "0.00997499999999986\n",
      "0.009975000000000053\n",
      "0.009975000000000026\n",
      "0.009975000000000008\n",
      "0.009974999999999993\n",
      "0.009974999999999982\n",
      "0.009974999999999973\n",
      "0.009974999999999967\n",
      "0.009974999999999961\n",
      "0.009974999999999956\n",
      "0.009974999999999953\n",
      "0.00997499999999995\n",
      "0.00997499999999995\n",
      "0.00997499999999995\n",
      "0.009974999999999946\n",
      "0.009974999999999946\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n",
      "0.009975000000000166\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(Parameter containing:\n",
       " tensor(0.9950, dtype=torch.float64, requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor(-2.4672e-17, dtype=torch.float64, requires_grad=True))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class Short(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Short, self).__init__()\n",
    "        self.y = nn.Parameter(torch.tensor(0.01, requires_grad=True, dtype=torch.double))#nn.Parameter(torch.randn(1, requires_grad=True, dtype=torch.double))\n",
    "        self.x = nn.Parameter(torch.tensor(1.1, requires_grad=True, dtype=torch.double))#nn.Parameter(torch.randn(1, requires_grad=True, dtype=torch.double)/100)\n",
    "        self.t1 = nn.Parameter(torch.tensor(0, requires_grad=True, dtype=torch.double))\n",
    "        self.t2 = nn.Parameter(torch.tensor(0, requires_grad=True, dtype=torch.double))\n",
    "        self.q1 = nn.Parameter(torch.tensor(0, requires_grad=True, dtype=torch.double))\n",
    "        self.q2 = nn.Parameter(torch.tensor(0, requires_grad=True, dtype=torch.double))\n",
    "        \n",
    "    def forward(self):\n",
    "        T1 = torch.cos(self.t1)\n",
    "        T2 = torch.cos(self.t2)\n",
    "        Q1 = torch.cos(self.q1)\n",
    "        Q2 = torch.cos(self.q2)\n",
    "        T12 = torch.cos(self.t1 - self.t2)\n",
    "        Q12 = torch.cos(self.q1 - self.q2)\n",
    "        return 1 + self.x**2 + self.y**2 - 2 * self.x.abs() * T1 * Q1 - 2 * self.y.abs() * T2 * Q2 + 2 * self.x.abs() * self.y.abs() * T12 * Q12\n",
    "    \n",
    "\n",
    "model = Short()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.1)  # Consider reducing the learning rate\n",
    "\n",
    "for i in range(1000):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    loss = model().sum()  # Changed to call the forward method without an argument\n",
    "    loss+= (model.x.abs().sum()+model.y.abs().sum())/100\n",
    "    \n",
    "    loss.backward()  # This is necessary to compute the gradients\n",
    "    optimizer.step()\n",
    "\n",
    "    if (i+1)%100:\n",
    "        print(loss.item())\n",
    "\n",
    "\n",
    "model.x,model.y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "tn=torch.zeros([2,3,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "lorentz_gens=torch.zeros([6,4,4], dtype=torch.float32)\n",
    "sig_mat=torch.tensor([[0,-1],[1,0]],requires_grad=False, dtype=torch.float32).view(-1)\n",
    "\n",
    "# Rotations\n",
    "lorentz_gens[0,[2,2,3,3],[2,3,2,3]]=sig_mat\n",
    "lorentz_gens[1,[3,3,1,1],[3,1,3,1]]=sig_mat\n",
    "lorentz_gens[2,[1,1,2,2],[1,2,1,2]]=sig_mat\n",
    "\n",
    "# Bosts\n",
    "lorentz_gens[3,[0,0,1,1],[0,1,0,1]]=sig_mat.abs()\n",
    "lorentz_gens[4,[0,0,2,2],[0,2,0,2]]=sig_mat.abs()\n",
    "lorentz_gens[5,[0,0,3,3],[0,3,0,3]]=sig_mat.abs()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 4])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(11.2790, grad_fn=<PowBackward0>),\n",
       " tensor(11.2790, grad_fn=<SumBackward0>),\n",
       " tensor(11.2790, grad_fn=<SumBackward0>))"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Create an example input tensor of shape [d, e, f]\n",
    "# inputs = torch.randn(2, 3, 4, requires_grad=True)\n",
    "\n",
    "torch.norm(inputs.view(-1),2)**2, (inputs**2).sum(), (torch.norm(inputs,2,dim=-1)**2).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.autograd import grad\n",
    "import torch.nn.functional as F\n",
    "\n",
    "sys.path.append('../python')\n",
    "\n",
    " \n",
    "from Lorentz_loss import LorentzLoss\n",
    "\n",
    "\n",
    "\n",
    "sys.path.append('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import grad\n",
    "import einops\n",
    "\n",
    "class LorentzInvariantModel(nn.Module):\n",
    "    \"\"\"A simple model that outputs a Lorentz scalar x^T  x.\"\"\"\n",
    "    def __init__(self):\n",
    "        super(LorentzInvariantModel, self).__init__()\n",
    "        # Minkowski metric \n",
    "        self.eta = torch.tensor([1, -1, -1, -1], dtype=torch.float32)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Reshape x to [B, N, 4] where N is number of 4-vectors\n",
    "        x = einops.rearrange(x, 'B (N d) -> B N d', d=4)\n",
    "        # Compute x^T  x for each 4-vector\n",
    "        return torch.einsum('B n i, i, B k i -> B n k', x, self.eta, x).view(B,-1).sum(dim=0)\n",
    "\n",
    "# Initialize the device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Initialize the model\n",
    "model = LorentzInvariantModel().to(device)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-28.,  -8.],\n",
       "        [-46.,  -2.]])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test input tensor (Batch size = 2, each input is 4-dimensional, 2 particles)\n",
    "test_input = torch.tensor([[1.0, 2.0, 3.0, 4.0, 2.0, 2.0, 2.0, 2.0], \n",
    "                           [2.0, 3.0, 4.0, 5.0, 1.0, 1.0, 1.0, 1.0]], dtype=torch.float32, device=device)\n",
    "model(test_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 2])\n",
      "torch.Size([2, 8])\n",
      "torch.Size([2, 2, 4])\n",
      "Lorentz Scalar Loss: 0.0\n"
     ]
    }
   ],
   "source": [
    "# Lorentz loss instance for scalar output\n",
    "lorentz_loss = LorentzLoss(device=device, model=model)\n",
    "\n",
    "\n",
    "test_input.requires_grad = True\n",
    "\n",
    "# Compute Lorentz loss for scalar case\n",
    "loss = lorentz_loss(test_input, spin='scalar')\n",
    "\n",
    "print(\"Lorentz Scalar Loss:\", loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lasso",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
