{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dc6eb6ea-6dd0-4167-b600-236b8ff669aa",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.autograd import grad\n",
    "import einops\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "import pickle\n",
    "import os\n",
    "import glob\n",
    "from symm_loss_defs import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "fbf8f0bb-d505-4eea-bcce-32432db164c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SymmLoss_pT_eta_phi(nn.Module):\n",
    "\n",
    "    def __init__(self,model, gens_list = [\"Lx\", \"Ly\", \"Lz\", \"Kx\", \"Ky\", \"Kz\"],device = devicef):\n",
    "        super(SymmLoss_pT_eta_phi, self).__init__()\n",
    "        \n",
    "        self.model = model.to(device)\n",
    "        self.device = device\n",
    "        \n",
    "        # Initialize generators (in future add different reps for inputs?)\n",
    "        GenList_names = []\n",
    "        Lorentz_names = [\"Lx\", \"Ly\", \"Lz\", \"Kx\", \"Ky\", \"Kz\"]\n",
    "        for gen in gens_list:\n",
    "            if gen in Lorentz_names:\n",
    "                GenList_names.append(gen)\n",
    "            else:\n",
    "                print(f\"generator \\n {gen} needs to be one of: {Lorentz_names}\") #This is for now. Later will add a part that deals with calculating the transforamtion for a given generator. \n",
    "                \n",
    "                # self.generators = einops.rearrange(gens_list, 'n w h -> n w h')\n",
    "                # self.generators = self.generators.to(device)\n",
    "        self.generators = GenList_names\n",
    "        \n",
    "        \n",
    "\n",
    "    def forward(self, input, model_rep='scalar',norm = \"none\",nfeatures = \"\",eta_linlog = \"lin\",phi_linlog = \"lin\"):\n",
    "        \n",
    "        input = input.clone().detach().requires_grad_(True)\n",
    "        input = input.to(self.device)\n",
    "        if nfeatures!=\"\":\n",
    "            dim = nfeatures\n",
    "        else:\n",
    "            dim = 4 #self.generators.shape[-1]\n",
    "        #Assuming input is shape [B,d*N] d is the number of features, N is the number of particles\n",
    "        input_reshaped = einops.rearrange(input, '... (N d) -> ... N d',d = dim)\n",
    "        \n",
    "        E = input[:,0::dim] #assuming input features are ordered as (E,pT,eta,phi)\n",
    "        \n",
    "        pT = input[:,1::dim]\n",
    "        \n",
    "        eta = input[:,2::dim]\n",
    "        if eta_linlog == \"log\":\n",
    "            eta = torch.exp(eta)\n",
    "        \n",
    "        phi = input[:,3::dim]\n",
    "        if phi_linlog == \"log\":\n",
    "            phi = torch.exp(phi)\n",
    "        \n",
    "        \n",
    "        GenList = self.generators  \n",
    "        \n",
    "        #dvar/dp L p, \n",
    "        ngen = len(self.generators)\n",
    "        dE = torch.zeros_like(E).to(self.device)\n",
    "        dpT = torch.zeros_like(pT).to(self.device)\n",
    "        deta = torch.zeros_like(eta).to(self.device)\n",
    "        dphi = torch.zeros_like(phi).to(self.device)\n",
    "        \n",
    "        \n",
    "        #Here for all the Lorentz generators. Later can add options for only some of them.\n",
    "        dE   = {\"Lx\": torch.zeros_like(E),              \"Ly\": torch.zeros_like(E),                \"Lz\":  torch.zeros_like(E),  \"Kx\":pT*torch.cos(phi),                    \"Ky\":pT*torch.sin(phi),                    \"Kz\":pT*torch.sinh(eta)}\n",
    "        dpT  = {\"Lx\": pT*torch.sin(phi)*torch.sinh(eta),\"Ly\": -pT*torch.cos(phi)*torch.sinh(eta), \"Lz\":  torch.zeros_like(pT), \"Kx\":E*torch.cos(phi),                     \"Ky\":E*torch.sin(phi),                     \"Kz\":torch.zeros_like(pT)}\n",
    "        deta = {\"Lx\": -1*torch.sin(phi)*torch.cosh(eta),  \"Ly\": torch.cos(phi)*torch.cosh(eta),     \"Lz\":  torch.zeros_like(eta),\"Kx\":-E*torch.cos(phi)*torch.tanh(eta)/pT, \"Ky\":-E*torch.sin(phi)*torch.tanh(eta)/pT, \"Kz\":E/(pT*torch.cosh(eta))}\n",
    "        dphi = {\"Lx\":  torch.cos(phi)*torch.sinh(eta),  \"Ly\": torch.sin(phi)*torch.sinh(eta),     \"Lz\":-1*torch.ones_like(phi),\"Kx\":-E*torch.sin(phi)/pT,                 \"Ky\":E*torch.cos(phi)/pT,                  \"Kz\":torch.zeros_like(phi)}\n",
    "        \n",
    "        \n",
    "        varsE = torch.empty(ngen,E.shape[0],E.shape[1]).to(self.device)\n",
    "        varspT = torch.empty(ngen,E.shape[0],E.shape[1]).to(self.device)\n",
    "        varseta = torch.empty(ngen,E.shape[0],E.shape[1]).to(self.device)\n",
    "        varsphi = torch.empty(ngen,E.shape[0],E.shape[1]).to(self.device)\n",
    "            \n",
    "        for i,gen in enumerate(GenList):\n",
    "            varsE[i] = dE[GenList[i]]\n",
    "            varspT[i] = dpT[GenList[i]]\n",
    "            varseta[i] = deta[GenList[i]]/eta if eta_linlog == \"log\" else deta[GenList[i]]\n",
    "            varsphi[i] = dphi[GenList[i]]/phi if phi_linlog == \"log\" else dphi[GenList[i]]\n",
    "        \n",
    "        varsSymm = torch.stack((varsE,varspT,varseta,varsphi), dim = -1) #[n,B,N,d]\n",
    "        #print(varsSymm.shape)\n",
    "            \n",
    "        # Compute model output, shape [B]\n",
    "        output = self.model(input_reshaped)\n",
    "\n",
    "        # Compute gradients with respect to input, shape [B, d*N], B is the batch size, d is the input irrep dimension, N is the number of particles\n",
    "        grads_input, = torch.autograd.grad(outputs=output, inputs=input, grad_outputs=torch.ones_like(output, device=self.device), create_graph=True)\n",
    "        \n",
    "        # Reshape grads to [B, N, d] \n",
    "        grads_input = einops.rearrange(grads_input, '... (N d) -> ... N d',d = dim)\n",
    "\n",
    "            \n",
    "        # Dot with input [n ,B]\n",
    "        differential_trans = torch.einsum('n ... N, ... N -> n ...', varsSymm, grads_input)\n",
    "        \n",
    "        scalar_loss = (differential_trans ** 2).mean()\n",
    "            \n",
    "            #add norm part here?\n",
    "     \n",
    "            \n",
    "        return scalar_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "53cf618e-83b3-4bd0-8a74-a6e6e0bb56b2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class inv_model_pT_eta_phi(nn.Module):\n",
    "\n",
    "    def __init__(self,dinput = 4, doutput = 1,init = \"rand\"):\n",
    "        super(inv_model_pT_eta_phi,self).__init__()\n",
    "        \n",
    "\n",
    "        bi_tensor = torch.randn(dinput,dinput)\n",
    "\n",
    "        if init==\"eta\":\n",
    "            diag = torch.ones(dinput)*(-1.00)\n",
    "            diag[0]=1.00\n",
    "            bi_tensor = torch.diag(diag)\n",
    "\n",
    "        elif init==\"delta\":\n",
    "            bi_tensor = torch.diag(torch.ones(dinput)*1.00)\n",
    "        \n",
    "        \n",
    "        bi_tensor = ((bi_tensor+torch.transpose(bi_tensor,0,1))*0.5).to(devicef)\n",
    "        self.bi_tensor = torch.nn.Parameter(bi_tensor)\n",
    "        self.bi_tensor.requires_grad_()\n",
    "\n",
    "    def forward(self,x, sig = \"euc\", d = 3 ):\n",
    "        #y = x @ (self.bi_tensor @ x.T)\n",
    "        x = x.to(devicef)\n",
    "        E = x[:,0::4]\n",
    "        #print(E)\n",
    "        pT = x[:,1::4]\n",
    "        eta = x[:,2::4]\n",
    "        phi = x[:,3::4]\n",
    "        px = pT*torch.cos(phi)\n",
    "        py = pT*torch.sin(phi)\n",
    "        pz = pT*torch.tanh(eta)/torch.sqrt(1-torch.tanh(eta)**2)\n",
    "        \n",
    "        #z = torch.transpose(torch.stack((torch.transpose(E),torch.transpose(pT),torch.transpose(eta),torch.transpose(phi)),dim = 1))\n",
    "        z = torch.cat((E,px,py,pz),dim = 1).to(devicef)\n",
    "        print(z)\n",
    "        y = torch.einsum(\"...i,ij,...j-> ...\",z,self.bi_tensor,z)\n",
    "       \n",
    "        return y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "4cddd41a-b655-4a2d-9cdb-7f77d68a3d81",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class inv_model_pT_eta_phi_dim(nn.Module):\n",
    "\n",
    "    def __init__(self,dinput = 4, doutput = 1,init = \"rand\"):\n",
    "        super(inv_model_pT_eta_phi_dim,self).__init__()\n",
    "        \n",
    "\n",
    "        bi_tensor = torch.randn(dinput,dinput)\n",
    "\n",
    "        if init==\"eta\":\n",
    "            diag = torch.ones(dinput)*(-1.00)\n",
    "            diag[0]=1.00\n",
    "            bi_tensor = torch.diag(diag)\n",
    "\n",
    "        elif init==\"delta\":\n",
    "            bi_tensor = torch.diag(torch.ones(dinput)*1.00)\n",
    "        \n",
    "        \n",
    "        bi_tensor = ((bi_tensor+torch.transpose(bi_tensor,0,1))*0.5).to(devicef)\n",
    "        self.bi_tensor = torch.nn.Parameter(bi_tensor)\n",
    "        self.bi_tensor.requires_grad_()\n",
    "\n",
    "    def forward(self,x, sig = \"euc\", d = 3 ):\n",
    "        #y = x @ (self.bi_tensor @ x.T)\n",
    "        x = x.to(devicef)\n",
    "        print(x)\n",
    "        E = x[:,:,:,0]\n",
    "        \n",
    "        pT = x[:,:,:,1]\n",
    "        eta = x[:,:,:,2]\n",
    "        phi = x[:,:,:,3]\n",
    "        px = pT*torch.cos(phi)\n",
    "        py = pT*torch.sin(phi)\n",
    "        pz = pT*torch.tanh(eta)/torch.sqrt(1-torch.tanh(eta)**2)\n",
    "        \n",
    "        #z = torch.transpose(torch.stack((torch.transpose(E),torch.transpose(pT),torch.transpose(eta),torch.transpose(phi)),dim = 1))\n",
    "        ps = torch.cat((E,px,py,pz),dim = 1).to(devicef)\n",
    "        z = ps.sum(dim=1)\n",
    "        print(z)\n",
    "        y = torch.einsum(\"...i,ij,...j-> ...\",z,self.bi_tensor,z)\n",
    "       \n",
    "        return y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "id": "22e41133-931a-4876-b49f-7564193787cd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dinput = 4\n",
    "N = 100\n",
    "norm = 1\n",
    "\n",
    "train_data = (torch.rand(N,dinput)-0.5)*norm\n",
    "train_dataset = TensorDataset(train_data)\n",
    "train_loader = DataLoader(train_dataset, batch_size=N, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "id": "50ece923-3e58-43d6-9d54-b6b29e80ec91",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "E = train_data[:,0::4]\n",
    "px =train_data[:,1::4]\n",
    "py = train_data[:,2::4]\n",
    "pz = train_data[:,3::4]\n",
    "pT = torch.sqrt(px**2+py**2)\n",
    "eta = torch.arctanh(pz/torch.sqrt(px**2+py**2+pz**2))\n",
    "phi = torch.arctan(py/px)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "id": "f8424f8b-e5e5-4107-a1b9-8b680c6db6a5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_data_prime = torch.stack((E,pT,eta,phi),dim=1).squeeze()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "id": "2f736c95-521a-4774-9bf7-e9cccd2df933",
   "metadata": {},
   "outputs": [],
   "source": [
    "mymodelLorentz = inv_model_pT_eta_phi(dinput = 4, init = \"eta\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "id": "c2258292-a904-4d17-9b77-a4fd36fbc627",
   "metadata": {},
   "outputs": [],
   "source": [
    "lossLorentz = SymmLoss_pT_eta_phi(model = mymodelLorentz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "id": "e2751bc6-afff-4474-9091-a1c723c46722",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "loss_res = lossLorentz(input = train_data_prime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "id": "a06b4ff9-19e3-47f7-b9a7-1b9a1ae73d98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(9.8683e-16, device='cuda:0', grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(loss_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "id": "c9d467bf-9731-45b5-89e3-52c9dc38e286",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mymodelLorentz_orig = inv_model(dinput = 4, init = \"eta\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "id": "74934f83-5ad1-40fb-b887-014a687d335d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "lossLorentz_orig = SymmLoss(gens_list=gens_Lorentz, model = mymodelLorentz_orig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "id": "120db6cc-c505-48d0-803c-c27e383957f3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.4956, -0.2114, -0.4526, -0.6944, -0.2188, -0.4268, -0.6869, -3.2348,\n",
       "        -2.0263, -2.5780, -2.1771, -2.4627, -0.7361, -1.7357, -0.3375, -2.2630,\n",
       "        -1.4821, -3.2321, -1.8447, -2.6270, -1.3080, -0.2570, -0.9055, -1.1938,\n",
       "        -0.9377, -0.0368, -1.5147, -0.0163, -0.9473, -0.8020, -1.4284, -1.0142,\n",
       "        -0.2353, -0.3589, -0.9659, -1.0337, -1.4175, -2.6716, -0.5331, -0.9565,\n",
       "        -0.0633, -1.2289, -3.7621, -0.4080, -7.5633, -3.6914, -3.0746, -1.8080,\n",
       "        -3.3444, -0.3049, -0.9497, -1.2709, -0.9815, -1.4976, -0.9070, -4.1121,\n",
       "        -2.6181, -3.1166, -1.8257, -0.6951, -1.0641, -0.5842, -0.7990, -2.7878,\n",
       "        -0.3988, -2.3080, -0.5472, -3.2336, -0.6079, -2.0571, -1.3465, -0.4643,\n",
       "        -3.0727, -1.2150, -1.0939, -0.9017, -1.7119, -1.8762, -1.6534, -2.9313,\n",
       "        -0.7373, -1.0086, -2.7413, -2.0608, -0.2969, -3.9602, -0.8981, -1.1807,\n",
       "        -0.5664, -1.3759, -0.3659, -0.7868, -3.5042, -1.0727, -1.1628, -0.8147,\n",
       "        -1.0165, -2.7074, -1.5103, -1.1619], device='cuda:0',\n",
       "       grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mymodelLorentz_orig(train_data.to(devicef))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "id": "f2c2793c-2bd0-4d20-926d-b4e0da1e1a31",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.4956, -0.2114, -0.4526, -0.6944, -0.2188, -0.4268, -0.6869, -3.2348,\n",
       "        -2.0263, -2.5780, -2.1771, -2.4627, -0.7361, -1.7357, -0.3375, -2.2630,\n",
       "        -1.4821, -3.2321, -1.8447, -2.6270, -1.3080, -0.2570, -0.9055, -1.1938,\n",
       "        -0.9377, -0.0368, -1.5147, -0.0163, -0.9473, -0.8020, -1.4284, -1.0142,\n",
       "        -0.2353, -0.3589, -0.9659, -1.0337, -1.4175, -2.6716, -0.5331, -0.9565,\n",
       "        -0.0633, -1.2289, -3.7621, -0.4080, -7.5633, -3.6914, -3.0746, -1.8080,\n",
       "        -3.3444, -0.3049, -0.9497, -1.2709, -0.9815, -1.4976, -0.9070, -4.1121,\n",
       "        -2.6181, -3.1166, -1.8257, -0.6951, -1.0641, -0.5842, -0.7990, -2.7878,\n",
       "        -0.3988, -2.3080, -0.5472, -3.2336, -0.6079, -2.0571, -1.3465, -0.4643,\n",
       "        -3.0727, -1.2150, -1.0939, -0.9017, -1.7119, -1.8762, -1.6534, -2.9313,\n",
       "        -0.7373, -1.0086, -2.7413, -2.0608, -0.2969, -3.9602, -0.8981, -1.1807,\n",
       "        -0.5664, -1.3759, -0.3659, -0.7868, -3.5042, -1.0727, -1.1628, -0.8147,\n",
       "        -1.0165, -2.7074, -1.5103, -1.1619], device='cuda:0',\n",
       "       grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 255,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mymodelLorentz(train_data_prime.to(devicef))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "id": "1e5b7cba-3eac-4f98-929b-3c077539c5b8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "loss_res_orig = lossLorentz_orig(input = train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "id": "987cd393-197e-46ca-a5ee-1f8cad89e26f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(6.2248e-18, device='cuda:0', grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(loss_res_orig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "deb2c67a-a1eb-4ab6-bf72-9a69f5fae754",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "p = torch.randn(100,10,3)\n",
    "E = p.norm(dim=-1)\n",
    "P = torch.cat([E.unsqueeze(-1),p],dim=-1).unsqueeze(0)\n",
    "train_data = P.clone().to(devicef)\n",
    "train_dataset = TensorDataset(train_data)\n",
    "train_loader = DataLoader(train_dataset, batch_size=100, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "bd80c626-add7-4832-a8ce-9197ed864813",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "E = train_data[:,:,:,0]\n",
    "px =train_data[:,:,:,1]\n",
    "py = train_data[:,:,:,2]\n",
    "pz = train_data[:,:,:,3]\n",
    "pT = torch.sqrt(px**2+py**2)\n",
    "eta = torch.arctanh(pz/torch.sqrt(px**2+py**2+pz**2))\n",
    "phi = torch.arctan(py/px)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "25034c59-33b5-49e6-972f-a75e732a2a09",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "flatten() got an unexpected keyword argument 'dim'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[53], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m train_data_prime \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mE\u001b[49m\u001b[43m,\u001b[49m\u001b[43mpT\u001b[49m\u001b[43m,\u001b[49m\u001b[43meta\u001b[49m\u001b[43m,\u001b[49m\u001b[43mphi\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflatten\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: flatten() got an unexpected keyword argument 'dim'"
     ]
    }
   ],
   "source": [
    "train_data_prime = torch.stack((E,pT,eta,phi),dim=-1).squeeze().flatten(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "859e1f16-ae21-4f46-abf3-e73bddf997cb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (3) must match the size of tensor b (2) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[54], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m mymodelLorentz \u001b[38;5;241m=\u001b[39m inv_model_pT_eta_phi_dim(dinput \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m4\u001b[39m, init \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meta\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      2\u001b[0m lossLorentz \u001b[38;5;241m=\u001b[39m SymmLoss_pT_eta_phi(model \u001b[38;5;241m=\u001b[39m mymodelLorentz)\n\u001b[0;32m----> 3\u001b[0m loss_res \u001b[38;5;241m=\u001b[39m \u001b[43mlossLorentz\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtrain_data_prime\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(loss_res)\n",
      "File \u001b[0;32m/global/common/software/nersc/pm-2022q4/sw/pytorch/2.0.1/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[51], line 59\u001b[0m, in \u001b[0;36mSymmLoss_pT_eta_phi.forward\u001b[0;34m(self, input, model_rep, norm, nfeatures, eta_linlog, phi_linlog)\u001b[0m\n\u001b[1;32m     55\u001b[0m dphi \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros_like(phi)\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m     58\u001b[0m \u001b[38;5;66;03m#Here for all the Lorentz generators. Later can add options for only some of them.\u001b[39;00m\n\u001b[0;32m---> 59\u001b[0m dE   \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLx\u001b[39m\u001b[38;5;124m\"\u001b[39m: torch\u001b[38;5;241m.\u001b[39mzeros_like(E),              \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLy\u001b[39m\u001b[38;5;124m\"\u001b[39m: torch\u001b[38;5;241m.\u001b[39mzeros_like(E),                \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLz\u001b[39m\u001b[38;5;124m\"\u001b[39m:  torch\u001b[38;5;241m.\u001b[39mzeros_like(E),  \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mKx\u001b[39m\u001b[38;5;124m\"\u001b[39m:\u001b[43mpT\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcos\u001b[49m\u001b[43m(\u001b[49m\u001b[43mphi\u001b[49m\u001b[43m)\u001b[49m,                    \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mKy\u001b[39m\u001b[38;5;124m\"\u001b[39m:pT\u001b[38;5;241m*\u001b[39mtorch\u001b[38;5;241m.\u001b[39msin(phi),                    \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mKz\u001b[39m\u001b[38;5;124m\"\u001b[39m:pT\u001b[38;5;241m*\u001b[39mtorch\u001b[38;5;241m.\u001b[39msinh(eta)}\n\u001b[1;32m     60\u001b[0m dpT  \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLx\u001b[39m\u001b[38;5;124m\"\u001b[39m: pT\u001b[38;5;241m*\u001b[39mtorch\u001b[38;5;241m.\u001b[39msin(phi)\u001b[38;5;241m*\u001b[39mtorch\u001b[38;5;241m.\u001b[39msinh(eta),\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLy\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m-\u001b[39mpT\u001b[38;5;241m*\u001b[39mtorch\u001b[38;5;241m.\u001b[39mcos(phi)\u001b[38;5;241m*\u001b[39mtorch\u001b[38;5;241m.\u001b[39msinh(eta), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLz\u001b[39m\u001b[38;5;124m\"\u001b[39m:  torch\u001b[38;5;241m.\u001b[39mzeros_like(pT), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mKx\u001b[39m\u001b[38;5;124m\"\u001b[39m:E\u001b[38;5;241m*\u001b[39mtorch\u001b[38;5;241m.\u001b[39mcos(phi),                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mKy\u001b[39m\u001b[38;5;124m\"\u001b[39m:E\u001b[38;5;241m*\u001b[39mtorch\u001b[38;5;241m.\u001b[39msin(phi),                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mKz\u001b[39m\u001b[38;5;124m\"\u001b[39m:torch\u001b[38;5;241m.\u001b[39mzeros_like(pT)}\n\u001b[1;32m     61\u001b[0m deta \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLx\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;241m*\u001b[39mtorch\u001b[38;5;241m.\u001b[39msin(phi)\u001b[38;5;241m*\u001b[39mtorch\u001b[38;5;241m.\u001b[39mcosh(eta),  \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLy\u001b[39m\u001b[38;5;124m\"\u001b[39m: torch\u001b[38;5;241m.\u001b[39mcos(phi)\u001b[38;5;241m*\u001b[39mtorch\u001b[38;5;241m.\u001b[39mcosh(eta),     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLz\u001b[39m\u001b[38;5;124m\"\u001b[39m:  torch\u001b[38;5;241m.\u001b[39mzeros_like(eta),\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mKx\u001b[39m\u001b[38;5;124m\"\u001b[39m:\u001b[38;5;241m-\u001b[39mE\u001b[38;5;241m*\u001b[39mtorch\u001b[38;5;241m.\u001b[39mcos(phi)\u001b[38;5;241m*\u001b[39mtorch\u001b[38;5;241m.\u001b[39mtanh(eta)\u001b[38;5;241m/\u001b[39mpT, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mKy\u001b[39m\u001b[38;5;124m\"\u001b[39m:\u001b[38;5;241m-\u001b[39mE\u001b[38;5;241m*\u001b[39mtorch\u001b[38;5;241m.\u001b[39msin(phi)\u001b[38;5;241m*\u001b[39mtorch\u001b[38;5;241m.\u001b[39mtanh(eta)\u001b[38;5;241m/\u001b[39mpT, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mKz\u001b[39m\u001b[38;5;124m\"\u001b[39m:E\u001b[38;5;241m/\u001b[39m(pT\u001b[38;5;241m*\u001b[39mtorch\u001b[38;5;241m.\u001b[39mcosh(eta))}\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (3) must match the size of tensor b (2) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "mymodelLorentz = inv_model_pT_eta_phi_dim(dinput = 4, init = \"eta\")\n",
    "lossLorentz = SymmLoss_pT_eta_phi(model = mymodelLorentz)\n",
    "loss_res = lossLorentz(input = train_data_prime)\n",
    "print(loss_res)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-2.0.1",
   "language": "python",
   "name": "pytorch-2.0.1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
